{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook assumes execution will be done in a Google Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WLlPT5ellIfy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from typing import Tuple, Optional\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a14um6lQ_Zuh"
   },
   "source": [
    "## Loading and preprocessing the four datasets used in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vcj6zpmBH2ik",
    "outputId": "5dfe640d-2741-462a-a79f-f1f2ee12f837"
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"/content/datasets\"):\n",
    "    os.mkdir(\"/content/datasets\")\n",
    "\n",
    "if not os.path.isdir(\"/content/datasets/DSIFN\"):\n",
    "    if os.path.exists(\"/content/DSIFN.zip\"):\n",
    "      !unzip /content/DSIFN.zip -d /content/datasets/DSIFN\n",
    "\n",
    "if not os.path.isdir(\"/content/datasets/WHU\"):\n",
    "    if os.path.exists(\"/content/WHU.zip\"):\n",
    "      !unzip /content/WHU.zip -d /content/datasets/WHU\n",
    "\n",
    "if not os.path.isdir(\"/content/datasets/LEVIR\"):\n",
    "    if os.path.exists(\"/content/LEVIR.zip\"):\n",
    "      !unzip /content/LEVIR.zip -d /content/datasets/LEVIR\n",
    "\n",
    "if not os.path.isdir(\"/content/datasets/CDD\"):\n",
    "    if os.path.exists(\"/content/CDD.zip\"):\n",
    "      !unzip /content/CDD.zip -d /content/datasets/CDD\n",
    "\n",
    "def load_DSIFN(percent_samples):\n",
    "\n",
    "    before = []\n",
    "    after = []\n",
    "    masks = []\n",
    "\n",
    "    dataset_path = \"/content/datasets/DSIFN\"\n",
    "\n",
    "    before_images = {os.path.basename(img): os.path.join(dataset_path + \"/A\", img) for img in os.listdir(dataset_path + \"/A\") if img.endswith(('.jpg', '.png'))}\n",
    "    after_images = {os.path.basename(img): os.path.join(dataset_path + \"/B\", img) for img in os.listdir(dataset_path + \"/B\") if img.endswith(('.jpg', '.png'))}\n",
    "    mask_images = {os.path.basename(img): os.path.join(dataset_path + \"/label\", img) for img in os.listdir(dataset_path + \"/label\") if img.endswith(('.jpg', '.png'))}\n",
    "\n",
    "    before_images = sorted(before_images)\n",
    "    after_images = sorted(after_images)\n",
    "    mask_images = sorted(mask_images)\n",
    "\n",
    "    common_files = set(before_images) & set(after_images) & set(mask_images)\n",
    "\n",
    "\n",
    "    before.extend(dataset_path + \"/A/\" + before_images[i] for i in range(len(common_files)))\n",
    "    after.extend(dataset_path + \"/B/\" + after_images[i] for i in range(len(common_files)))\n",
    "    masks.extend(dataset_path + \"/label/\" + mask_images[i] for i in range(len(common_files)))\n",
    "\n",
    "    all_data = pd.DataFrame(\n",
    "        {\"Before Image\": before,\n",
    "         \"After Image\": after,\n",
    "         \"GT Mask\": masks}\n",
    "        )\n",
    "    all_data[\"Image ID\"] = all_data[\"Before Image\"].str.split(\"/\").str[-1]\n",
    "    all_data[\"Dataset\"] = \"DSIFN\"\n",
    "\n",
    "    # Make train test val splits\n",
    "    temp_data, test_data = train_test_split(all_data, test_size=0.15, random_state=42)\n",
    "    val_size = 0.15 / (1 - 0.15)\n",
    "    train_data, val_data = train_test_split(temp_data, test_size=val_size, random_state=42)\n",
    "\n",
    "    # Shuffle the datasets\n",
    "    train_data = train_data.sample(frac=percent_samples).reset_index(drop=True)\n",
    "    val_data = val_data.sample(frac=percent_samples).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=percent_samples).reset_index(drop=True)\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def load_WHU(percent_samples):\n",
    "\n",
    "    before = []\n",
    "    after = []\n",
    "    masks = []\n",
    "\n",
    "    dataset_path = \"/content/datasets/WHU\"\n",
    "\n",
    "    before_images = {os.path.basename(img): os.path.join(dataset_path + \"/A\", img) for img in os.listdir(dataset_path + \"/A\") if img.endswith(('.jpg', '.png'))}\n",
    "    after_images = {os.path.basename(img): os.path.join(dataset_path + \"/B\", img) for img in os.listdir(dataset_path + \"/B\") if img.endswith(('.jpg', '.png'))}\n",
    "    mask_images = {os.path.basename(img): os.path.join(dataset_path + \"/label\", img) for img in os.listdir(dataset_path + \"/label\") if img.endswith(('.jpg', '.png'))}\n",
    "\n",
    "    before_images = sorted(before_images)\n",
    "    after_images = sorted(after_images)\n",
    "    mask_images = sorted(mask_images)\n",
    "\n",
    "    common_files = set(before_images) & set(after_images) & set(mask_images)\n",
    "\n",
    "\n",
    "    before.extend(dataset_path + \"/A/\" + before_images[i] for i in range(len(common_files)))\n",
    "    after.extend(dataset_path + \"/B/\" + after_images[i] for i in range(len(common_files)))\n",
    "    masks.extend(dataset_path + \"/label/\" + mask_images[i] for i in range(len(common_files)))\n",
    "\n",
    "    all_data = pd.DataFrame(\n",
    "        {\"Before Image\": before,\n",
    "         \"After Image\": after,\n",
    "         \"GT Mask\": masks}\n",
    "        )\n",
    "    all_data[\"Image ID\"] = all_data[\"Before Image\"].str.split(\"_\").str[-1]\n",
    "    all_data[\"Dataset\"] = \"WHU\"\n",
    "\n",
    "    # Make train test val splits\n",
    "    temp_data, test_data = train_test_split(all_data, test_size=0.15, random_state=42)\n",
    "    val_size = 0.15 / (1 - 0.15)\n",
    "    train_data, val_data = train_test_split(temp_data, test_size=val_size, random_state=42)\n",
    "\n",
    "    train_data.shape\n",
    "\n",
    "    # Shuffle the datasets\n",
    "    train_data = train_data.sample(frac=percent_samples).reset_index(drop=True)\n",
    "    val_data = val_data.sample(frac=percent_samples).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=percent_samples).reset_index(drop=True)\n",
    "\n",
    "    train_data.shape\n",
    "    train_data.head()\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def load_LEVIR(percent_samples):\n",
    "\n",
    "    before_train = []\n",
    "    after_train = []\n",
    "    masks_train = []\n",
    "\n",
    "    before_val = []\n",
    "    after_val = []\n",
    "    masks_val = []\n",
    "\n",
    "    before_test = []\n",
    "    after_test = []\n",
    "    masks_test = []\n",
    "\n",
    "    for dataset_type in [\"train_\", \"val_\", \"test_\"]:\n",
    "        dataset_path = \"/content/datasets/LEVIR\"\n",
    "\n",
    "        before_images = {os.path.basename(img): os.path.join(dataset_path + \"/A\", img) for img in os.listdir(dataset_path + \"/A\") if img.startswith(dataset_type) and img.endswith(('.jpg', '.png'))}\n",
    "        after_images = {os.path.basename(img): os.path.join(dataset_path + \"/B\", img) for img in os.listdir(dataset_path + \"/B\") if img.startswith(dataset_type) and img.endswith(('.jpg', '.png'))}\n",
    "        mask_images = {os.path.basename(img): os.path.join(dataset_path + \"/label\", img) for img in os.listdir(dataset_path + \"/label\") if img.startswith(dataset_type) and img.endswith(('.jpg', '.png'))}\n",
    "\n",
    "        before_images = sorted(before_images)\n",
    "        after_images = sorted(after_images)\n",
    "        mask_images = sorted(mask_images)\n",
    "\n",
    "        common_files = set(before_images) & set(after_images) & set(mask_images)\n",
    "\n",
    "        if dataset_type == \"train_\":\n",
    "            before_train.extend(dataset_path + \"/A/\" + before_images[i] for i in range(len(common_files)))\n",
    "            after_train.extend(dataset_path + \"/B/\" + after_images[i] for i in range(len(common_files)))\n",
    "            masks_train.extend(dataset_path + \"/label/\" + mask_images[i] for i in range(len(common_files)))\n",
    "        elif dataset_type == \"val_\":\n",
    "            before_val.extend(dataset_path + \"/A/\" + before_images[i] for i in range(len(common_files)))\n",
    "            after_val.extend(dataset_path + \"/B/\" + after_images[i] for i in range(len(common_files)))\n",
    "            masks_val.extend(dataset_path + \"/label/\" + mask_images[i] for i in range(len(common_files)))\n",
    "        elif dataset_type == \"test_\":\n",
    "            before_test.extend(dataset_path + \"/A/\" + before_images[i] for i in range(len(common_files)))\n",
    "            after_test.extend(dataset_path + \"/B/\" + after_images[i] for i in range(len(common_files)))\n",
    "            masks_test.extend(dataset_path + \"/label/\" + mask_images[i] for i in range(len(common_files)))\n",
    "\n",
    "    train_data = pd.DataFrame(\n",
    "        {\"Before Image\": before_train,\n",
    "         \"After Image\": after_train,\n",
    "         \"GT Mask\": masks_train}\n",
    "        )\n",
    "    train_data[\"Image ID\"] = \"_\" + train_data[\"Before Image\"].str.split(\"_\").str[-2] + \"_\" + train_data[\"Before Image\"].str.split(\"_\").str[-1]\n",
    "    train_data[\"Dataset\"] = \"LEVIR\"\n",
    "\n",
    "    val_data = pd.DataFrame(\n",
    "        {\"Before Image\": before_val,\n",
    "         \"After Image\": after_val,\n",
    "         \"GT Mask\": masks_val}\n",
    "         )\n",
    "    val_data[\"Image ID\"] = val_data[\"Before Image\"].str.split(\"_\").str[-2] + \"_\" + val_data[\"Before Image\"].str.split(\"_\").str[-1]\n",
    "    val_data[\"Dataset\"] = \"LEVIR\"\n",
    "\n",
    "    test_data = pd.DataFrame(\n",
    "        {\"Before Image\": before_test,\n",
    "         \"After Image\": after_test,\n",
    "         \"GT Mask\": masks_test}\n",
    "        )\n",
    "    test_data[\"Image ID\"] = test_data[\"Before Image\"].str.split(\"_\").str[-2] + \"_\" + test_data[\"Before Image\"].str.split(\"_\").str[-1]\n",
    "    test_data[\"Dataset\"] = \"LEVIR\"\n",
    "\n",
    "    # Shuffle the datasets\n",
    "    train_data = train_data.sample(frac=percent_samples).reset_index(drop=True)\n",
    "    val_data = val_data.sample(frac=percent_samples).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=percent_samples).reset_index(drop=True)\n",
    "\n",
    "    train_data.shape\n",
    "    train_data.head()\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def load_CDD(percent_samples):\n",
    "\n",
    "    before_train = []\n",
    "    after_train = []\n",
    "    masks_train = []\n",
    "\n",
    "    before_val = []\n",
    "    after_val = []\n",
    "    masks_val = []\n",
    "\n",
    "    before_test = []\n",
    "    after_test = []\n",
    "    masks_test = []\n",
    "\n",
    "    for dataset_type in [\"train\", \"val\", \"test\"]:\n",
    "        dataset_path = os.path.join(\"/content/datasets/CDD/CDD\", dataset_type)\n",
    "\n",
    "        before_images = {os.path.basename(img): os.path.join(dataset_path + \"/A\", img) for img in os.listdir(dataset_path + \"/A\") if img.startswith(dataset_type) and img.endswith(('.jpg', '.png'))}\n",
    "        after_images = {os.path.basename(img): os.path.join(dataset_path + \"/B\", img) for img in os.listdir(dataset_path + \"/B\") if img.startswith(dataset_type) and img.endswith(('.jpg', '.png'))}\n",
    "        mask_images = {os.path.basename(img): os.path.join(dataset_path + \"/OUT\", img) for img in os.listdir(dataset_path + \"/OUT\") if img.startswith(dataset_type) and img.endswith(('.jpg', '.png'))}\n",
    "\n",
    "        before_images = sorted(before_images)\n",
    "        after_images = sorted(after_images)\n",
    "        mask_images = sorted(mask_images)\n",
    "\n",
    "        common_files = set(before_images) & set(after_images) & set(mask_images)\n",
    "\n",
    "        if dataset_type == \"train\":\n",
    "            before_train.extend(dataset_path + \"/A/\" + before_images[i] for i in range(len(common_files)))\n",
    "            after_train.extend(dataset_path + \"/B/\" + after_images[i] for i in range(len(common_files)))\n",
    "            masks_train.extend(dataset_path + \"/OUT/\" + mask_images[i] for i in range(len(common_files)))\n",
    "        elif dataset_type == \"val\":\n",
    "            before_val.extend(dataset_path + \"/A/\" + before_images[i] for i in range(len(common_files)))\n",
    "            after_val.extend(dataset_path + \"/B/\" + after_images[i] for i in range(len(common_files)))\n",
    "            masks_val.extend(dataset_path + \"/OUT/\" + mask_images[i] for i in range(len(common_files)))\n",
    "        elif dataset_type == \"test\":\n",
    "            before_test.extend(dataset_path + \"/A/\" + before_images[i] for i in range(len(common_files)))\n",
    "            after_test.extend(dataset_path + \"/B/\" + after_images[i] for i in range(len(common_files)))\n",
    "            masks_test.extend(dataset_path + \"/OUT/\" + mask_images[i] for i in range(len(common_files)))\n",
    "\n",
    "    train_data = pd.DataFrame(\n",
    "        {\"Before Image\": before_train,\n",
    "         \"After Image\": after_train,\n",
    "         \"GT Mask\": masks_train}\n",
    "        )\n",
    "    print(train_data.shape[0])\n",
    "    train_data[\"Image ID\"] = train_data[\"Before Image\"].str.split(\"_\").str[-1]\n",
    "    train_data[\"Dataset\"] = \"CDD\"\n",
    "\n",
    "    val_data = pd.DataFrame(\n",
    "        {\"Before Image\": before_val,\n",
    "         \"After Image\": after_val,\n",
    "         \"GT Mask\": masks_val}\n",
    "         )\n",
    "    val_data[\"Image ID\"] = val_data[\"Before Image\"].str.split(\"_\").str[-1]\n",
    "    val_data[\"Dataset\"] = \"CDD\"\n",
    "\n",
    "    test_data = pd.DataFrame(\n",
    "        {\"Before Image\": before_test,\n",
    "         \"After Image\": after_test,\n",
    "         \"GT Mask\": masks_test}\n",
    "        )\n",
    "    test_data[\"Image ID\"] = test_data[\"Before Image\"].str.split(\"_\").str[-1]\n",
    "    test_data[\"Dataset\"] = \"CDD\"\n",
    "\n",
    "    # Shuffle the datasets\n",
    "    train_data = train_data.sample(frac=percent_samples).reset_index(drop=True)\n",
    "    val_data = val_data.sample(frac=percent_samples).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=percent_samples).reset_index(drop=True)\n",
    "\n",
    "    train_data.shape\n",
    "    train_data.head()\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# You can combine at least two dataset splits and up to all four dataset splits\n",
    "def combine_dataset_splits(\n",
    "    train1: pd.DataFrame, val1: pd.DataFrame, test1: pd.DataFrame,\n",
    "    train2: pd.DataFrame, val2: pd.DataFrame, test2: pd.DataFrame,\n",
    "    train3: Optional[pd.DataFrame] = None,\n",
    "    val3: Optional[pd.DataFrame] = None,\n",
    "    test3: Optional[pd.DataFrame] = None,\n",
    "    train4: Optional[pd.DataFrame] = None,\n",
    "    val4: Optional[pd.DataFrame] = None,\n",
    "    test4: Optional[pd.DataFrame] = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    train_list = [train1, train2]\n",
    "    val_list = [val1, val2]\n",
    "    test_list = [test1, test2]\n",
    "\n",
    "    if train3 is not None:\n",
    "        train_list.append(train3)\n",
    "        val_list.append(val3)\n",
    "        test_list.append(test3)\n",
    "\n",
    "    if train4 is not None:\n",
    "        train_list.append(train4)\n",
    "        val_list.append(val4)\n",
    "        test_list.append(test4)\n",
    "\n",
    "    train_combined = pd.concat(train_list, ignore_index=True)\n",
    "    val_combined = pd.concat(val_list, ignore_index=True)\n",
    "    test_combined = pd.concat(test_list, ignore_index=True)\n",
    "\n",
    "    train_combined = train_combined.sample(frac=1).reset_index(drop=True)\n",
    "    val_combined = val_combined.sample(frac=1).reset_index(drop=True)\n",
    "    test_combined = test_combined.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    return train_combined, val_combined, test_combined\n",
    "\n",
    "# Function to display a desired number of images from the dataset (Before image, after image, and gt mask)\n",
    "def display_before_after_mask(images, num_samples):\n",
    "\n",
    "  for i in range(num_samples):\n",
    "\n",
    "    before = images.iloc[i]['Before Image']\n",
    "    after = images.iloc[i]['After Image']\n",
    "    mask = images.iloc[i]['GT Mask']\n",
    "\n",
    "    before_img = cv2.imread(before)\n",
    "    after_img = cv2.imread(after)\n",
    "    mask_img = cv2.imread(mask, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    before_img = cv2.cvtColor(before_img, cv2.COLOR_BGR2RGB)\n",
    "    after_img = cv2.cvtColor(after_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(before_img)\n",
    "    plt.title(\"Before Image: \" + images.iloc[i]['Dataset'] + \" \" + images.iloc[i]['Image ID'])\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(after_img)\n",
    "    plt.title(\"After Image: \" + images.iloc[i]['Dataset'] + \" \" + images.iloc[i]['Image ID'])\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(mask_img, cmap='gray')\n",
    "    plt.title(\"GT Mask: \" + images.iloc[i]['Dataset'] + \" \" + images.iloc[i]['Image ID'])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The datasets to use can be adjusted by the user\n",
    "- Used two of the four in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEVIR_train, LEVIR_val, LEVIR_test = load_LEVIR(1.0)\n",
    "# CDD_train, CDD_val, CDD_test = load_CDD(0.15)\n",
    "# WHU_train, WHU_val, WHU_test = load_WHU(1.0)\n",
    "DSIFN_train, DSIFN_val, DSIFN_test = load_DSIFN(1.0)\n",
    "\n",
    "# combined_train, combined_val, combined_test = combine_dataset_splits(\n",
    "#     CDD_train, CDD_val, CDD_test,\n",
    "#     DSIFN_train, DSIFN_val, DSIFN_test,\n",
    "#     LEVIR_train, LEVIR_val, LEVIR_test,\n",
    "#     WHU_train, WHU_val, WHU_test\n",
    "#   )\n",
    "combined_train, combined_val, combined_test = combine_dataset_splits(\n",
    "    DSIFN_train, DSIFN_val, DSIFN_test,\n",
    "    LEVIR_train, LEVIR_val, LEVIR_test\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_tensors(ds, target_size=(256, 256)):\n",
    "    before_paths = ds['Before Image'].tolist()\n",
    "    after_paths = ds['After Image'].tolist()\n",
    "    gt_paths = ds['GT Mask'].tolist()\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((before_paths, after_paths, gt_paths))\n",
    "\n",
    "    def load_and_preprocess(before_path, after_path, gt_path):\n",
    "        before = tf.io.read_file(before_path)\n",
    "        before = tf.image.decode_png(before, channels=3)\n",
    "        before = tf.image.resize(before, target_size) / 255.0\n",
    "\n",
    "        after = tf.io.read_file(after_path)\n",
    "        after = tf.image.decode_png(after, channels=3)\n",
    "        after = tf.image.resize(after, target_size) / 255.0\n",
    "\n",
    "        gt = tf.io.read_file(gt_path)\n",
    "        gt = tf.image.decode_png(gt, channels=1)\n",
    "        gt = tf.image.resize(gt, target_size) / 255.0\n",
    "\n",
    "        return {\"pre_images\": before, \"post_images\": after}, gt\n",
    "\n",
    "    dataset = dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(4).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# # Combined dataset\n",
    "combined_tensor_train = map_to_tensors(combined_train)\n",
    "combined_tensor_val = map_to_tensors(combined_val)\n",
    "combined_tensor_test = map_to_tensors(combined_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5oqd_VenLE4"
   },
   "source": [
    "Change number of desired epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCSYVrs9nPXI"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1Nr-4YE-idq"
   },
   "source": [
    "Showing a few example before-after images with their ground truth masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "01mV8_d2-hTs",
    "outputId": "f586473b-de8e-4d46-8b5d-cbd16a1afa67"
   },
   "outputs": [],
   "source": [
    "display_before_after_mask(combined_train, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qvAtwp4byYw1"
   },
   "outputs": [],
   "source": [
    "# Stem Module (Start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48gYm5V0pxG3"
   },
   "outputs": [],
   "source": [
    "# Defining the Stem Module; based on the authors' approach, dimensions = [96, 192, 384, 768]\n",
    "def stem_module(input_shape=(256, 256, 3), embedding_dim=768):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Applying a series of 2D convolutional layers (4 layers according to authors approach); based on the authors' approach, dimensions = [96, 192, 384, 768]\n",
    "    x1 = layers.Conv2D(96, (7, 7), padding='same', activation='relu')(inputs)  # Conv Layer 1\n",
    "    x1p = layers.MaxPooling2D(pool_size=(2, 2))(x1)  # 128x128\n",
    "\n",
    "    x2 = layers.Conv2D(192, (3, 3), padding='same', activation='relu')(x1p)  # Conv Layer 2\n",
    "    x2p = layers.MaxPooling2D(pool_size=(2, 2))(x2)  # 64x64\n",
    "\n",
    "    x3 = layers.Conv2D(384, (3, 3), padding='same', activation='relu')(x2p)  # Conv Layer 3\n",
    "    x3p = layers.MaxPooling2D(pool_size=(2, 2))(x3)  # 32x32\n",
    "\n",
    "    x4 = layers.Conv2D(embedding_dim, (3, 3), padding='same', activation='relu')(x3p)  # Conv Layer 4\n",
    "    x4p = layers.MaxPooling2D(pool_size=(2, 2))(x4)  # 16x16\n",
    "\n",
    "    return models.Model(inputs=inputs, outputs=[x1, x2, x3, x4p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sM8aheAdMKcF"
   },
   "source": [
    "In the paper it says the Stem Module (SM) operates similarly to a Vision Transformer (ViT). Should this basic SM not work as we thought, we can probably substitute for a pretrained ViT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abb8Iye2zMqo"
   },
   "outputs": [],
   "source": [
    "# Stem Module (End)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fbf7brtD8fsJ"
   },
   "source": [
    "Feeding the inputs to the stem modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FOVu3dYq6sG"
   },
   "outputs": [],
   "source": [
    "pre_images = layers.Input(shape=(256, 256, 3), name=\"pre_images\")\n",
    "post_images = layers.Input(shape=(256, 256, 3), name=\"post_images\")\n",
    "\n",
    "stem = stem_module(input_shape=(256, 256, 3))\n",
    "pre_skip1, pre_skip2, pre_skip3, pre_image_features = stem(pre_images)\n",
    "post_skip1, post_skip2, post_skip3, post_image_features = stem(post_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeX4dL5eFA_p"
   },
   "source": [
    "The Stem Module should be functioning as intended now. These features are will be passed to the VSS blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8dXmSPSL1Lu"
   },
   "outputs": [],
   "source": [
    "# Siamese Encoder + Difference Module Implementation (Start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFDY5NfdL1ZG"
   },
   "outputs": [],
   "source": [
    "class SelectiveScan2D(layers.Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(SelectiveScan2D, self).__init__(**kwargs)\n",
    "            # Parameters A, B, C, D, and delta are learnable parameters\n",
    "            self.A = self.add_weight(name='A', shape=(1,), initializer='random_normal', trainable=True)\n",
    "            self.B = self.add_weight(name='B', shape=(1,), initializer='random_normal', trainable=True)\n",
    "            self.C = self.add_weight(name='C', shape=(1,), initializer='random_normal', trainable=True)\n",
    "            self.D = self.add_weight(name='D', shape=(1,), initializer='random_normal', trainable=True)\n",
    "            self.delta = self.add_weight(name='delta', shape=(1,), initializer='random_normal', trainable=True)\n",
    "\n",
    "        def call(self, features):\n",
    "          # Assuming input is a 4D tensor of shape (batch_size, height, width, channels)\n",
    "          # batch_size, height, width, channels = tf.shape(features)\n",
    "\n",
    "          shape = tf.shape(features)\n",
    "          batch_size = shape[0]\n",
    "          height = shape[1]\n",
    "          width = shape[2]\n",
    "          channels = shape[3]\n",
    "\n",
    "          # Flatten the input into a 2D tensor (batch_size, height * width * channels)\n",
    "          flattened_input = tf.reshape(features, (batch_size, -1))\n",
    "\n",
    "          # Reshape back to 2D (height, width, channels) for processing each scan direction\n",
    "          features_2d = tf.reshape(features, (batch_size, height, width, channels))\n",
    "\n",
    "          # Processing top-left to bottom-right (Scan Direction A)\n",
    "          scan_tl_br = self.A * features_2d + self.delta\n",
    "\n",
    "          # Processing bottom-right to top-left (Scan Direction B)\n",
    "          scan_br_tl = self.B * tf.reverse(features_2d, axis=[1, 2]) + self.delta\n",
    "\n",
    "          # Processing top-right to bottom-left (Scan Direction C)\n",
    "          scan_tr_bl = self.C * tf.reverse(features_2d, axis=[2]) + self.delta\n",
    "\n",
    "          # Processing bottom-left to top-right (Scan Direction D)\n",
    "          scan_bl_tr = self.D * tf.reverse(features_2d, axis=[1]) + self.delta\n",
    "\n",
    "          # Merge results (we simply add them; can possibly experiment with different mergings later)\n",
    "          merge_scans = scan_tl_br + scan_br_tl + scan_tr_bl + scan_bl_tr\n",
    "\n",
    "          return merge_scans\n",
    "\n",
    "class DifferenceModule(layers.Layer):\n",
    "    def __init__(self, ss2D, **kwargs):\n",
    "        super(DifferenceModule, self).__init__(**kwargs)\n",
    "        self.ss2D = ss2D\n",
    "\n",
    "        self.conv1 = layers.Conv2D(64, kernel_size=1, activation='relu')\n",
    "\n",
    "        self.depthwise_conv = layers.DepthwiseConv2D(kernel_size=(3, 3), padding='same', activation='relu')\n",
    "\n",
    "        # Joint Selective Scan 2D Module\n",
    "        self.joint_selective_scan_2D = JointSelectiveScan2D(self.ss2D)\n",
    "\n",
    "        self.layernorm = layers.LayerNormalization()\n",
    "\n",
    "        self.dense2 = layers.Dense(64, activation='relu')\n",
    "\n",
    "        self.pool = layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='same')\n",
    "\n",
    "    def call(self, pre_post_features):\n",
    "        pre, post = pre_post_features\n",
    "\n",
    "        pre = self.conv1(pre)\n",
    "        post = self.conv1(post)\n",
    "\n",
    "        pre = self.depthwise_conv(pre)\n",
    "        post = self.depthwise_conv(post)\n",
    "\n",
    "        # Applying joint selective scan 2D\n",
    "        pre, post = self.joint_selective_scan_2D([pre, post])\n",
    "\n",
    "        pre = self.layernorm(pre)\n",
    "        post = self.layernorm(post)\n",
    "\n",
    "        # Concatenating pre and post and passing it through a linear layer\n",
    "        combined = layers.Concatenate(axis=-1)([pre, post])\n",
    "        combined = self.dense2(combined)\n",
    "\n",
    "        return combined\n",
    "\n",
    "class JointSelectiveScan2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, ss2D, **kwargs):\n",
    "        super(JointSelectiveScan2D, self).__init__(**kwargs)\n",
    "        self.ss2D = ss2D\n",
    "        self.A = ss2D.A\n",
    "        self.B = ss2D.B\n",
    "        self.C = ss2D.C\n",
    "        self.D = ss2D.D\n",
    "        self.delta = ss2D.delta\n",
    "\n",
    "    def call(self, pre_post_features):\n",
    "        pre_feat, post_feat = pre_post_features\n",
    "\n",
    "        # Extracting shapes\n",
    "        batch_size = tf.shape(pre_feat)[0]\n",
    "        height = tf.shape(pre_feat)[1]\n",
    "        width = tf.shape(pre_feat)[2]\n",
    "        channels = tf.shape(pre_feat)[3]\n",
    "\n",
    "        # Preprocessing\n",
    "        pre_post_concat = tf.concat([pre_feat, post_feat], axis=-1)  # [B, H, W, 2C]\n",
    "        post_pre_concat = tf.concat([post_feat, pre_feat], axis=-1)  # [B, H, W, 2C]\n",
    "\n",
    "        # Defining a scan function\n",
    "        def selective_scan(input_feat):\n",
    "            scan_tl_br = self.A * input_feat + self.delta\n",
    "            scan_br_tl = self.B * tf.reverse(input_feat, axis=[1, 2]) + self.delta\n",
    "            scan_tr_bl = self.C * tf.reverse(input_feat, axis=[2]) + self.delta\n",
    "            scan_bl_tr = self.D * tf.reverse(input_feat, axis=[1]) + self.delta\n",
    "            return scan_tl_br + scan_br_tl + scan_tr_bl + scan_bl_tr\n",
    "\n",
    "        # Apply scan to both concatenations\n",
    "        pre_output = selective_scan(pre_post_concat)\n",
    "        post_output = selective_scan(post_pre_concat)\n",
    "\n",
    "        return pre_output, post_output\n",
    "\n",
    "class VSSBlock(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(VSSBlock, self).__init__(**kwargs)\n",
    "\n",
    "        # Linear Layer (we can change 128 if we want more feature space dimensionality)\n",
    "        self.dense1 = layers.Dense(64, activation='relu')\n",
    "\n",
    "        # Depth-wise Convolutional Layer\n",
    "        self.depthwise_conv = layers.DepthwiseConv2D(kernel_size=(3, 3), padding='same', activation='relu')\n",
    "\n",
    "        # Selective Scan 2D Module\n",
    "        self.selective_scan_2D = SelectiveScan2D()\n",
    "\n",
    "        # Difference Module\n",
    "        self.difference_module = DifferenceModule(self.selective_scan_2D)\n",
    "\n",
    "        # Layer Normalization\n",
    "        self.layernorm = layers.LayerNormalization()\n",
    "\n",
    "        # Linear Layer (final output)\n",
    "        self.dense2 = layers.Dense(64, activation='relu')\n",
    "\n",
    "        # Downsampling (maxPooling; we can change the method of downsampling later if necessary)\n",
    "        self.pool = layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='same')\n",
    "\n",
    "    def call(self, pre_post_features, dm_vectors = None):\n",
    "      if dm_vectors is None:\n",
    "        dm_vectors = []\n",
    "      pre_image_features = pre_post_features[0]\n",
    "      post_image_features = pre_post_features[1]\n",
    "\n",
    "      for i in range(4):\n",
    "        # Processing pre-image and post-image feature vectors\n",
    "        pre = self.dense1(pre_image_features)  # Applying first linear transformation\n",
    "        post = self.dense1(post_image_features)\n",
    "\n",
    "        batch_size_pre = tf.shape(pre)[0]\n",
    "        pre = tf.reshape(pre, (batch_size_pre, 8, 8, 256))  # Reshaping to simulate spatial dimensions for the conv layer (adjust as needed)\n",
    "\n",
    "        batch_size_post = tf.shape(post)[0]\n",
    "        post = tf.reshape(post, (batch_size_post, 8, 8, 256))\n",
    "\n",
    "        pre = self.depthwise_conv(pre)  # Applying depth-wise convolution\n",
    "        post = self.depthwise_conv(post)\n",
    "        pre = self.selective_scan_2D(pre)  # Applying SS2D\n",
    "        post = self.selective_scan_2D(post)\n",
    "        pre = self.layernorm(pre)  # Applying layer normalization\n",
    "        post = self.layernorm(post)\n",
    "        pre = self.dense2(pre)  # Final linear layer (output)\n",
    "        post = self.dense2(post)\n",
    "        dm_vector = self.difference_module([pre, post]) # Applying Difference Module\n",
    "        dm_vectors.append(dm_vector)\n",
    "        if i < 3:\n",
    "          pre = self.pool(pre)  # Downsampling\n",
    "          post = self.pool(post)\n",
    "\n",
    "      # Returning feature vectors (from Difference Module) to be fed to Mask Decoder\n",
    "      dm_vectors = layers.Concatenate(axis=-1)(dm_vectors)\n",
    "\n",
    "      return dm_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5ePMvLdL1dd"
   },
   "outputs": [],
   "source": [
    "# Siamese Encoder + Difference Module Implementation (End)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2Xvovu4keSk"
   },
   "outputs": [],
   "source": [
    "vss = VSSBlock()\n",
    "dm_vectors = vss([pre_image_features, post_image_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahL6xOalB2Xi"
   },
   "outputs": [],
   "source": [
    "# Begin Mask Decoder Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f6Mpc9pDqK8b"
   },
   "outputs": [],
   "source": [
    "class CAVSSBlock(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CAVSSBlock, self).__init__(**kwargs)\n",
    "\n",
    "        self.vss_block = VSSBlockMaskDecoder()\n",
    "\n",
    "        self.norm = layers.LayerNormalization()\n",
    "        self.conv = layers.Conv2D(1, kernel_size=1)\n",
    "\n",
    "        self.avg_pool = layers.AveragePooling2D(pool_size=(2, 2), strides=1, padding='same')\n",
    "        self.max_pool = layers.MaxPooling2D(pool_size=(2, 2), strides=1, padding='same')\n",
    "\n",
    "        self.sigmoid = layers.Activation('sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        residual = x\n",
    "        avg = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
    "\n",
    "        x = self.vss_block(avg)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        avg_pool = self.avg_pool(x)\n",
    "        max_pool = self.max_pool(x)\n",
    "\n",
    "        combined = layers.Add()([avg_pool, max_pool])\n",
    "        gate = self.sigmoid(combined)\n",
    "\n",
    "        # Resize gate to match residual shape\n",
    "        gate_resized = tf.image.resize(gate, size=tf.shape(residual)[1:3], method='bilinear')\n",
    "\n",
    "        return gate_resized * residual\n",
    "\n",
    "class MaskDecoder(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MaskDecoder, self).__init__()\n",
    "        self.upsample1 = layers.UpSampling2D(size=(8, 8), interpolation='bilinear')\n",
    "        self.cavss1 = CAVSSBlock()\n",
    "\n",
    "        self.upsample2 = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')\n",
    "        self.cavss2 = CAVSSBlock()\n",
    "\n",
    "        self.upsample3 = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')\n",
    "        self.cavss3 = CAVSSBlock()\n",
    "\n",
    "        self.final_conv = layers.Conv2D(1, 1, activation='sigmoid')  # binary segmentation\n",
    "\n",
    "    def call(self, x, skip_features=None):\n",
    "\n",
    "        if skip_features:\n",
    "            for i, (upsample, cavss, (pre, post)) in enumerate(zip(\n",
    "                [self.upsample1, self.upsample2, self.upsample3],\n",
    "                [self.cavss1, self.cavss2, self.cavss3],\n",
    "                skip_features\n",
    "            )):\n",
    "                x = upsample(x)\n",
    "                skip = tf.concat([pre, post], axis=-1)\n",
    "                x = tf.concat([x, skip], axis=-1)\n",
    "                x = cavss(x)\n",
    "        else:\n",
    "            x = self.upsample1(x)\n",
    "            x = self.cavss1(x)\n",
    "            x = self.upsample2(x)\n",
    "            x = self.cavss2(x)\n",
    "            x = self.upsample3(x)\n",
    "            x = self.cavss3(x)\n",
    "\n",
    "        return self.final_conv(x)\n",
    "\n",
    "\n",
    "class VSSBlockMaskDecoder(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(VSSBlockMaskDecoder, self).__init__(**kwargs)\n",
    "        self.dense1 = layers.Dense(64, activation='relu')\n",
    "        self.depthwise_conv = layers.DepthwiseConv2D(kernel_size=(3, 3), padding='same', activation='relu')\n",
    "        self.selective_scan_2D = SelectiveScan2D()\n",
    "        self.layernorm = layers.LayerNormalization()\n",
    "        self.dense2 = layers.Dense(64, activation='relu')\n",
    "\n",
    "    def call(self, features):\n",
    "        x = self.dense1(features)\n",
    "        # Automatically infer shape from input\n",
    "        b, h, w, c = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "        x = tf.reshape(x, (b, h, w, c // 1))  # reshape shouldn't drop rank\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.selective_scan_2D(x)\n",
    "        x = self.layernorm(x)\n",
    "        x = self.dense2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QtyAYUDev_F"
   },
   "outputs": [],
   "source": [
    "# End Mask Decoder Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWYsW90A8JYh"
   },
   "source": [
    "Building the model and showing its summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "id": "IaqwVakAr4HP",
    "outputId": "f841dae0-08a4-4fa9-9d25-4850411c94b2"
   },
   "outputs": [],
   "source": [
    "decoder = MaskDecoder()\n",
    "output_mask = decoder(dm_vectors, skip_features=[\n",
    "    (pre_skip3, post_skip3),\n",
    "    (pre_skip2, post_skip2),\n",
    "    (pre_skip1, post_skip1)\n",
    "])\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.models.Model(inputs=[pre_images, post_images], outputs=output_mask)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGlos0Zbgsu6"
   },
   "source": [
    "Should output (256, 256, 1) corresponding to width (256) and height (256) of input images, and output channels (1) for black and white difference mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDEb7-1j8BMy"
   },
   "source": [
    "Instantiating the optimizer and loss function. Fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cp6YWnpufbv3",
    "outputId": "b3655229-8c83-4b6c-bdf7-12f643118eb5"
   },
   "outputs": [],
   "source": [
    "# Combo loss recommended for change detection. Authors did not specify what loss function they used.\n",
    "def combo_loss(y_true, y_pred):\n",
    "    # Ensure shapes match and types are consistent\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    # Binary Crossentropy\n",
    "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "    # Reduce per image to scalar\n",
    "    bce = tf.reduce_mean(bce, axis=[1, 2])\n",
    "\n",
    "    # Dice loss per image\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n",
    "    dice_loss = 1. - (2. * intersection + 1e-7) / (union + 1e-7)\n",
    "\n",
    "    # Combine losses\n",
    "    loss = 0.5 * bce + 0.5 * dice_loss\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# According to authors\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=6e-5, weight_decay=0.01)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=combo_loss, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(combined_tensor_train,\n",
    "          validation_data=combined_tensor_val,\n",
    "          epochs = EPOCHS,\n",
    "          callbacks=[callbacks.EarlyStopping(patience=3, restore_best_weights=True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWvWkTzH79yI"
   },
   "source": [
    "## Saving the model in two ways.\n",
    "- Uncomment cells to save the model if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ToDwxqyo_3iJ",
    "outputId": "2c14bc22-268c-4c39-8bf7-0650d785ced0"
   },
   "outputs": [],
   "source": [
    "#model.save(\"siamese_detection_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "beLqyBoCAEWp"
   },
   "outputs": [],
   "source": [
    "#model.save(\"siamese_detection_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n24ZXVU772Jt"
   },
   "source": [
    "## Showing a few of our predictions next to before and after images, as well as ground truth images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eynhTdObZcor",
    "outputId": "acf294fa-ee9c-44ca-ea74-f41a755a1b54"
   },
   "outputs": [],
   "source": [
    "for batch in combined_tensor_test.take(10):\n",
    "  inputs, ground_truth = batch\n",
    "\n",
    "  pred = model.predict(inputs)\n",
    "  pre_image = inputs[\"pre_images\"][0]\n",
    "  post_image = inputs[\"post_images\"][0]\n",
    "  pred_mask = pred[0]\n",
    "  pred_mask = tf.cast(pred_mask > 0.5, tf.int32)\n",
    "  true_mask = ground_truth[0]\n",
    "\n",
    "\n",
    "  fig, axs = plt.subplots(1, 4, figsize=(10, 5))\n",
    "\n",
    "  axs[0].imshow(tf.squeeze(pred_mask), cmap='gray')\n",
    "  axs[0].set_title(\"Predicted Mask\")\n",
    "\n",
    "  axs[1].imshow(tf.squeeze(true_mask), cmap='gray')\n",
    "  axs[1].set_title(\"Ground Truth\")\n",
    "\n",
    "  axs[2].imshow(tf.squeeze(pre_image), cmap='gray')\n",
    "  axs[2].set_title(\"Before Image\")\n",
    "\n",
    "  axs[3].imshow(tf.squeeze(post_image), cmap='gray')\n",
    "  axs[3].set_title(\"After Image\")\n",
    "\n",
    "  for ax in axs:\n",
    "    ax.axis('off')\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlh9wRug7nt3"
   },
   "source": [
    "## Getting F1, IoU, and Overall Accuracy Scores based on authors' evaluation metrics.\n",
    "\n",
    "### Additionally getting validation and validation accuracy plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZl8k1q4pvTK"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, jaccard_score, accuracy_score\n",
    "\n",
    "def plot_training_accuracy_curves(model, history, val_ds):\n",
    "\n",
    "    val_loss, val_accuracy = model.evaluate(val_ds)\n",
    "    print(f\"Validation loss: {val_loss}, Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, test_dataset):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for (inputs, y) in test_dataset:\n",
    "        pred = model.predict(inputs)\n",
    "        pred = tf.where(pred > 0.5, 1, 0).numpy()\n",
    "        y_pred.append(pred)\n",
    "        y_true.append(y.numpy())\n",
    "\n",
    "    y_true = np.concatenate(y_true).astype(np.uint8).flatten()\n",
    "    y_pred = np.concatenate(y_pred).astype(np.uint8).flatten()\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    iou = jaccard_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"IoU Score: {iou}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    return f1, iou, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "hyNAJb1xzU9W",
    "outputId": "54d788e8-3b4b-4c57-df7b-3d624b6382ed"
   },
   "outputs": [],
   "source": [
    "plot_training_accuracy_curves(model, history, combined_tensor_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-P_ynKO7AJz",
    "outputId": "56633a2e-6750-4bf4-d3d4-430a13811db9"
   },
   "outputs": [],
   "source": [
    "f1, iou, accuracy = evaluate_model(model, combined_tensor_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
